 ### Date: July 25, 2019
 
 ## **Challenges Faced**
 
Seed Mongo database with 10 million items
Learn how to use postgreSQL and seed that database as well
Learn faker for faking data sets



## **Actions Taken**
Researched the postgres docs and faker.io docs most of the day.
I was able to generate a CSV file containing 10 million items using a script to use faker and fs.writeFile. 
My first attempt was to loop 10 million times using fakers node module and fsWrite. My computer was not happy about this.
Ended up having to write 1 million items at time. 
From a CSV it only took about 20 seconds to seed the MongoDB with mongo import but I stripped the data pretty bare. 
looked something like this.
_id, Name, cat
100000, Silly Fake Name, Hardware
The collection took 387MB of space on the locally hosted Mongo database. 
Used the same CSV to seed the PostgreSQl database.
662MB in postgreSQL. 



## **Results/Take-aways**
I was surprised that the postgres database required more space even though I only put a single table in. 
I assume this has something to do with making it more accessible for look up time. 
After speaking with other team members it seems generating the data to a CSV was the optimal way to seed. 
Others tried writing to the database WHILE generating the data through a single script and it took 45 minutes or more depending on hardware.
